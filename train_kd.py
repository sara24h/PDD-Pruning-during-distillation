import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import datetime
from args import args
from data.Data import CIFAR10, CIFAR100
from resnet_kd_auto_prune import resnet20_auto, resnet56_auto
from trainer.trainer import validate, train_KD
from utils.utils import set_random_seed, set_gpu, Logger, get_logger


def ApproxSign(mask):
  
    out_forward = torch.sign(mask)
    mask1 = mask < -1
    mask2 = mask < 0
    mask3 = mask < 1
    out1 = (-1) * mask1.type(torch.float32) + (mask * mask + 2 * mask) * (1 - mask1.type(torch.float32))
    out2 = out1 * mask2.type(torch.float32) + (-mask * mask + 2 * mask) * (1 - mask2.type(torch.float32))
    out3 = out2 * mask3.type(torch.float32) + 1 * (1 - mask3.type(torch.float32))
    out = out_forward.detach() - out3.detach() + out3
    out = (out + 1) / 2  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ [0, 1]
    return out


def load_teacher_checkpoint(args):

    ckpt = None
    
    if args.arch == 'resnet56':
        if args.pretrained:
            if args.set == 'cifar10':
                print("="*80)
                print("Downloading ResNet-56 CIFAR-10 checkpoint...")
                print("="*80)
                checkpoint_url = 'https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar10_resnet56-187c023a.pt'
                try:
                    ckpt = torch.hub.load_state_dict_from_url(
                        checkpoint_url, 
                        map_location=f'cuda:{args.gpu}',
                        progress=True,
                        check_hash=True
                    )
                    print("âœ“ Checkpoint downloaded successfully!")
                except Exception as e:
                    print(f"âœ— Error: {e}")
                    raise
                        
            elif args.set == 'cifar100':
                checkpoint_url = 'https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar100_resnet56-f2eff4c8.pt'
                ckpt = torch.hub.load_state_dict_from_url(
                    checkpoint_url, 
                    map_location=f'cuda:{args.gpu}',
                    progress=True
                )
    
    return ckpt


def main():
    print(args)
    sys.stdout = Logger('print_process.log', sys.stdout)

    if args.random_seed is not None:
        set_random_seed(args.random_seed)

    main_worker(args)


def main_worker(args):

    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
    log_dir = f'pretrained_model/{args.arch}/{args.set}'
    os.makedirs(log_dir, exist_ok=True)
    logger = get_logger(f'{log_dir}/logger_{now}.log')
    
    logger.info("="*80)
    logger.info("Automatic Pruning Configuration:")
    logger.info(f"  Teacher: {args.arch}")
    logger.info(f"  Student: {args.arch_s}")
    logger.info(f"  Dataset: {args.set}")
    logger.info(f"  Pruning: AUTOMATIC (no manual channel config needed)")
    logger.info(f"  Pruning Threshold: {args.pruning_threshold if hasattr(args, 'pruning_threshold') else 0.0}")
    logger.info("="*80)

    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 1: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¯Ø§Ù†Ø´Ø¬Ùˆ Ø¨Ø§ Pruning Ø®ÙˆØ¯Ú©Ø§Ø±
    # ========================================================================
    print("\n" + "="*80)
    print("Creating Student Model with Automatic Pruning...")
    print("="*80)
    
    if args.arch_s == 'resnet20':
        # âœ… Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ in_cfg Ùˆ out_cfg Ù†Ø¯Ø§Ø±ÛŒØ¯!
        model_s = resnet20_auto(
            num_classes=args.num_classes,
            option='B',
            use_pruning=True  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ù‡Ø±Ø³ Ø®ÙˆØ¯Ú©Ø§Ø±
        )
        print("âœ“ Student model created: ResNet-20 (with automatic pruning)")
    else:
        raise ValueError(f"Unsupported student: {args.arch_s}")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 2: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ù…Ø¹Ù„Ù…
    # ========================================================================
    print("\n" + "="*80)
    print("Creating Teacher Model...")
    print("="*80)
    
    if args.arch == 'resnet56':
        model = resnet56_auto(
            num_classes=args.num_classes,
            option='B',
            use_pruning=False  # Ù…Ø¹Ù„Ù… Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ pruning Ù†Ø¯Ø§Ø±Ø¯
        )
        print("âœ“ Teacher model created: ResNet-56")
    else:
        raise ValueError(f"Unsupported teacher: {args.arch}")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 3: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ checkpoint Ù…Ø¹Ù„Ù…
    # ========================================================================
    if args.pretrained:
        print("\n" + "="*80)
        print("Loading Teacher Checkpoint...")
        print("="*80)
        ckpt = load_teacher_checkpoint(args)
        
        if ckpt is not None:
            # ØªØµØ­ÛŒØ­ Ù†Ø§Ù… Ú©Ù„ÛŒØ¯Ù‡Ø§
            new_ckpt = {}
            for key, value in ckpt.items():
                new_key = key
                if key.startswith('fc.'):
                    new_key = key.replace('fc.', 'linear.')
                elif 'downsample' in key:
                    new_key = key.replace('downsample', 'shortcut')
                new_ckpt[new_key] = value
            
            model.load_state_dict(new_ckpt, strict=False)
            print("âœ“ Teacher checkpoint loaded!")
    
    # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU
    model_s = set_gpu(args, model_s)
    model = set_gpu(args, model)
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 4: ÙØ±ÛŒØ² Ú©Ø±Ø¯Ù† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¹Ù„Ù…
    # ========================================================================
    print("\n" + "="*80)
    print("Freezing Teacher Parameters...")
    print("="*80)
    
    for param in model.parameters():
        param.requires_grad = False
    model.eval()
    print("âœ“ Teacher frozen")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 5: ØªØ¹Ø±ÛŒÙ ØªÙˆØ§Ø¨Ø¹ Loss
    # ========================================================================
    criterion = nn.CrossEntropyLoss().cuda()
    divergence_loss = F.kl_div
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 6: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
    # ========================================================================
    print("\n" + "="*80)
    print("Loading Dataset...")
    print("="*80)
    
    if args.set == 'cifar10':
        data = CIFAR10()
    elif args.set == 'cifar100':
        data = CIFAR100()
    else:
        raise ValueError(f"Unknown dataset: {args.set}")
    print(f"âœ“ Dataset loaded: {args.set.upper()}")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 7: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø¹Ù„Ù…
    # ========================================================================
    print("\n" + "="*80)
    print("Validating Teacher...")
    print("="*80)
    
    acc1, acc5 = validate(data.val_loader, model, criterion, args)
    print(f"Teacher Accuracy: Top-1={acc1:.2f}%, Top-5={acc5:.2f}%")
    logger.info(f"Teacher: Top-1={acc1:.2f}%, Top-5={acc5:.2f}%")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 8: ØªÙ†Ø¸ÛŒÙ… Optimizer Ùˆ Scheduler
    # ========================================================================
    print("\n" + "="*80)
    print("Setup Optimizer & Scheduler...")
    print("="*80)
    
    optimizer = torch.optim.SGD(
        model_s.parameters(), 
        lr=args.lr, 
        momentum=args.momentum, 
        weight_decay=args.weight_decay
    )
    
    lr_decay_step = list(map(int, args.lr_decay_step.split(',')))
    scheduler = torch.optim.lr_scheduler.MultiStepLR(
        optimizer, 
        milestones=lr_decay_step, 
        gamma=0.1
    )
    print(f"âœ“ SGD: lr={args.lr}, momentum={args.momentum}, wd={args.weight_decay}")
    print(f"âœ“ Scheduler: milestones={lr_decay_step}")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 9: Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
    # ========================================================================
    best_acc1 = 0.0
    best_acc5 = 0.0
    pruning_threshold = getattr(args, 'pruning_threshold', 0.0)
    
    print("\n" + "="*80)
    print(f"Starting Training with Automatic Pruning...")
    print(f"Total Epochs: {args.epochs}")
    print(f"Pruning Threshold: {pruning_threshold}")
    print("="*80)
    
    for epoch in range(args.start_epoch, args.epochs):
        print("\n" + "="*80)
        print(f"Epoch [{epoch+1}/{args.epochs}] - LR: {optimizer.param_groups[0]['lr']:.6f}")
        print("="*80)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ distillation
        train_acc1, train_acc5 = train_KD(
            data.train_loader, 
            model,      # Teacher
            model_s,    # Student
            divergence_loss, 
            criterion, 
            optimizer, 
            epoch, 
            args
        )
        
        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        acc1, acc5 = validate(data.val_loader, model_s, criterion, args)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ learning rate
        scheduler.step()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬
        is_best = acc1 > best_acc1
        best_acc1 = max(acc1, best_acc1)
        best_acc5 = max(acc5, best_acc5)
        
        # Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡
        print(f"\nEpoch {epoch+1} Summary:")
        print(f"  Train: Top-1={train_acc1:.2f}%, Top-5={train_acc5:.2f}%")
        print(f"  Val:   Top-1={acc1:.2f}%, Top-5={acc5:.2f}%")
        print(f"  Best:  Top-1={best_acc1:.2f}%, Top-5={best_acc5:.2f}%")
        logger.info(f"Epoch {epoch+1}: Train={train_acc1:.2f}%, Val={acc1:.2f}%")
        
        # ====================================================================
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Pruning Ù‡Ø± 10 epoch
        # ====================================================================
        if (epoch + 1) % 10 == 0 or is_best:
            print("\n" + "-"*80)
            print("Current Pruning Statistics:")
            print("-"*80)
            model_s.print_pruning_stats(threshold=pruning_threshold)
        
        # ====================================================================
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        # ====================================================================
        if is_best:
            print(f"\n{'*'*80}")
            print(f"ðŸŽ‰ New Best Model! Accuracy: {acc1:.2f}%")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù‡Ø±Ø³â€ŒØ´Ø¯Ù‡
            arch_config = model_s.extract_pruned_architecture(threshold=pruning_threshold)
            print(f"\nPruned Architecture (threshold={pruning_threshold}):")
            print(f"  in_cfg:  {arch_config['in_cfg']}")
            print(f"  out_cfg: {arch_config['out_cfg']}")
            print(f"{'*'*80}\n")
            
            # Ø°Ø®ÛŒØ±Ù‡ checkpoint
            checkpoint = {
                'epoch': epoch + 1,
                'state_dict': model_s.state_dict(),
                'best_acc1': best_acc1,
                'best_acc5': best_acc5,
                'optimizer': optimizer.state_dict(),
                'arch_config': arch_config,
                'threshold': pruning_threshold
            }
            
            save_path = f'{log_dir}/best_model.pth'
            torch.save(checkpoint, save_path)
            print(f"âœ“ Best model saved to: {save_path}")
            
            logger.info(f"New best: Epoch={epoch+1}, Acc={acc1:.2f}%")
            logger.info(f"Architecture: in_cfg={arch_config['in_cfg']}")
            logger.info(f"Architecture: out_cfg={arch_config['out_cfg']}")
    
    # ========================================================================
    # Ù…Ø±Ø­Ù„Ù‡ 10: Ø§Ø¹Ù…Ø§Ù„ Ù‡Ø±Ø³ Ù†Ù‡Ø§ÛŒÛŒ
    # ========================================================================
    print("\n" + "="*80)
    print("Applying Final Pruning...")
    print("="*80)
    
    model_s.apply_pruning(threshold=pruning_threshold)
    
    # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
    final_acc1, final_acc5 = validate(data.val_loader, model_s, criterion, args)
    
    print("\n" + "="*80)
    print("ðŸŽŠ Training Completed!")
    print(f"Best Validation Accuracy: {best_acc1:.2f}%")
    print(f"Final Accuracy (after pruning): {final_acc1:.2f}%")
    print("="*80)
    
    logger.info("="*80)
    logger.info("Training Completed!")
    logger.info(f"Best Accuracy: {best_acc1:.2f}%")
    logger.info(f"Final Accuracy: {final_acc1:.2f}%")
    logger.info("="*80)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
    final_checkpoint = {
        'state_dict': model_s.state_dict(),
        'accuracy': final_acc1,
        'arch_config': model_s.extract_pruned_architecture(threshold=pruning_threshold),
        'threshold': pruning_threshold
    }
    
    final_path = f'{log_dir}/final_pruned_model.pth'
    torch.save(final_checkpoint, final_path)
    print(f"\nâœ“ Final pruned model saved to: {final_path}")


if __name__ == "__main__":
   
    main()
